{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zik1Z3gHtjYW"
      },
      "source": [
        "# Simple Naive Bayes Classifier\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggYBPhcJuuAu"
      },
      "source": [
        "## T1. Load a dataset\n",
        "\n",
        "The following code loads a dataset consisting of text messages and spam-ham labels.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2napWf8tINR",
        "outputId": "fb3e3edc-090e-4da1-b616-5a8cd2f8ca59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ham     4825\n",
            "spam     747\n",
            "Name: Category, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "from typing import List, Tuple, Dict, Iterable, Set\n",
        "from collections import defaultdict\n",
        "import re\n",
        "import math\n",
        "import pandas as pd\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/mlee-pnu/IDS/main/spam_dataset.csv'\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# TODOs\n",
        "hams = df['Category'].value_counts()[\"ham\"]\n",
        "spams = df['Category'].value_counts()[\"spam\"]\n",
        "print(df['Category'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5wryED8lOBP"
      },
      "source": [
        "## T2. Spam filter for individual words\n",
        "\n",
        "We first defined a function ***tokenize()*** to convert a given text into a set of words. \n",
        "\n",
        "Using the function, we now try to count the frequency of each word in each class (spam and ham).\n",
        "\n",
        "Complete the following code and answer the following questions:\n",
        " \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j6rmDIBJnFNh"
      },
      "outputs": [],
      "source": [
        "def tokenize(text: str) -> Set[str]:\n",
        "    text = text.lower()                         \n",
        "    all_words = re.findall(\"[a-z0-9']+\", text)  \n",
        "    return set(all_words)                       "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_GtcK6qXlrES",
        "outputId": "f6934222-be52-4ad1-cb26-3aadb1b364a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.74235807860262 0.2576419213973799\n"
          ]
        }
      ],
      "source": [
        "tokens: Set[str] = set()\n",
        "token_spam_counts: Dict[str, int] = defaultdict(int)\n",
        "token_ham_counts: Dict[str, int] = defaultdict(int)\n",
        "\n",
        "spam = df[df.Category == 'spam']\n",
        "ham = df[df.Category == 'ham']\n",
        "\n",
        "spam_word_list = []\n",
        "\n",
        "for msg in spam['Message'].to_list():\n",
        "  for token in tokenize(msg):\n",
        "    tokens.add(token)\n",
        "    token_spam_counts[token] += 1\n",
        "    spam_word_list.append(token)\n",
        "\n",
        "for msg in ham['Message'].to_list():\n",
        "  for token in tokenize(msg):\n",
        "    tokens.add(token)\n",
        "    token_ham_counts[token] += 1\n",
        "\n",
        "from collections import Counter\n",
        "spam_dict = dict(Counter(spam_word_list))\n",
        "\n",
        "# TODOs\n",
        "word = \"free\"\n",
        "n_word_spam = token_spam_counts[\"free\"] # frequency of the word in spam messages\n",
        "n_word_ham =  token_ham_counts[\"free\"] # frequency of the word in ham messages\n",
        "\n",
        "# print(n_word_spam, n_word_ham)\n",
        "\n",
        "p_spam = spam['Message'].count()/df['Message'].count() # P(spam)\n",
        "p_ham = ham['Message'].count()/df['Message'].count() # P(ham)\n",
        "# print(p_spam, p_ham)\n",
        "p_word_given_spam = (n_word_spam/df['Message'].count())/p_spam # P(word|spam)\n",
        "p_word_given_ham = (n_word_ham/df['Message'].count())/p_ham # P(word|ham)\n",
        "# print(p_word_given_spam, p_word_given_ham)\n",
        "\n",
        "# p(spam|word)\n",
        "p_word = (n_word_ham+n_word_spam)\n",
        "p_spam_given_word = n_word_spam/p_word\n",
        "# P(ham|word)\n",
        "p_ham_given_word = n_word_ham/p_word\n",
        "print(p_spam_given_word, p_ham_given_word)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUzCC6l6knfU"
      },
      "source": [
        "## T3. Spam filter that combines words: Naive Bayes\n",
        "\n",
        "You received a text message \"just do it\" from an unknown sender.\n",
        "\n",
        "Complete the function ***predict()*** that outputs the probability of the message being spam and the predicted label of the message. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDTL_uBGL86f",
        "outputId": "a28fa771-6e46-4723-f824-041b1a92f13a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(5.132694869879663e-07, 'ham')\n"
          ]
        }
      ],
      "source": [
        "text = \"just do it\"\n",
        "\n",
        "# TODOs\n",
        "# solution 1.  \n",
        "def predict(text: str):\n",
        "  prob = 1\n",
        "  label = \"spam\"\n",
        "\n",
        "  k = 0.0 # smoothing factor\n",
        "  log_spam = log_ham = 0.0\n",
        "  \n",
        "  for token in tokens:\n",
        "    # Calculate p(token|spam), p(token|ham) \n",
        "    word = token\n",
        "    n_word_spam = token_spam_counts[word] # frequency of the word in spam messages\n",
        "    n_word_ham = token_ham_counts[word]   # frequency of the word in ham messages\n",
        "\n",
        "    p_spam = spams/(hams+spams)  # P(spam)\n",
        "    p_ham = hams/(hams+spams)    # P(ham)\n",
        "    p_word_given_spam = (n_word_spam + k) / (spams + 2*k)  # P(word|spam)\n",
        "    p_word_given_ham = (n_word_ham + k) / (hams + 2*k)     # P(word|ham)\n",
        "\n",
        "    # iterating on the bag of words \n",
        "    if token in tokenize(text):\n",
        "      log_spam += math.log(p_word_given_spam)\n",
        "      log_ham += math.log(p_word_given_ham)\n",
        "    else:\n",
        "      log_spam += math.log(1.0 - p_word_given_spam)\n",
        "      log_ham += math.log(1.0 - p_word_given_ham)\n",
        "\n",
        "  p_if_spam = math.exp(log_spam + math.log(p_spam))\n",
        "  p_if_ham = math.exp(log_ham + math.log(p_ham))\n",
        "  prob = p_if_spam / (p_if_spam + p_if_ham)\n",
        "  label = \"spam\" if prob > 0.5 else \"ham\"\n",
        "  return prob, label\n",
        "\n",
        "\n",
        "print(predict(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjQ3M91eTG7I"
      },
      "source": [
        "## T4. Smoothing method\n",
        "\n",
        "You again received two text messages from unknown senders.\n",
        "\n",
        "Complete the function ***spamFilter()*** that classifies a given message. \n",
        "\n",
        "You may want to apply a smoothing method for this task.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJ66DoXVTpR2",
        "outputId": "a6cb9083-767f-4087-f7f0-2ba3a02b5dda"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5.5983428595874115e-33 1.53351339674579e-32\n",
            "('ham', 0.267434927596669)\n",
            "7.855596234887694e-25 3.766074044359178e-16\n",
            "('ham', 2.085884697425939e-09)\n"
          ]
        }
      ],
      "source": [
        "########## OKAY BUT NOT CORRECT\n",
        "textA = \"reward! download your free ticket from our website www.pnu.edu\"\n",
        "textB = \"call me and get your money back\"\n",
        "\n",
        "# TODOs\n",
        "def spamFilter2(text: str):\n",
        "  k = 1.0 # smoothing factor\n",
        "  log_spam = log_ham = 0.0\n",
        "  \n",
        "  for token in tokens:\n",
        "    # Calculate p(token|spam), p(token|ham) \n",
        "    word = token\n",
        "    n_word_spam = token_spam_counts[word] # frequency of the word in spam messages\n",
        "    n_word_ham = token_ham_counts[word]   # frequency of the word in ham messages\n",
        "\n",
        "    p_spam = spams/(hams+spams)  # P(spam)\n",
        "    p_ham = hams/(hams+spams)    # P(ham)\n",
        "    p_word_given_spam = (n_word_spam + k) / (spams + 2*k)  # P(word|spam)\n",
        "    p_word_given_ham = (n_word_ham + k) / (hams + 2*k)     # P(word|ham)\n",
        "\n",
        "    # iterating on the bag of words \n",
        "    if token in tokenize(text):\n",
        "      log_spam += math.log(p_word_given_spam)\n",
        "      log_ham += math.log(p_word_given_ham)\n",
        "    else:\n",
        "      log_spam += math.log(1.0 - p_word_given_spam)\n",
        "      log_ham += math.log(1.0 - p_word_given_ham)\n",
        "\n",
        "  p_if_spam = math.exp(log_spam + math.log(p_spam))\n",
        "  p_if_ham = math.exp(log_ham + math.log(p_ham))\n",
        "  # p_if_spam = math.exp(log_spam)\n",
        "  # p_if_ham = math.exp(log_ham)\n",
        "  print(p_if_spam, p_if_ham)\n",
        "  prob =  p_if_spam / (p_if_spam + p_if_ham)\n",
        "  label = \"spam\" if prob > 0.5 else \"ham\" \n",
        "  return label, prob\n",
        "\n",
        "print(spamFilter2(textA))\n",
        "print(spamFilter2(textB))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "고태영 - HW 11-1 Spam Filter.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
